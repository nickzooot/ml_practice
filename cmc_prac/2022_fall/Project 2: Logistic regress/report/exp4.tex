\subsection{Эксперимент 4. Предобработка корпуса и ее влияние на качество модели.}
\subsubsection{Дизайн эксперимента}
В данном эксперименте использовались те же параметры, что и в предыдущем:
\begin{itemize}
	\item Для GD:
	\begin{enumerate}
		\item $\alpha = 0.1$
		\item  $\beta = 0.01$
		\item  $\omega_0 = 0 \in \mathbb{R}^D$
	\end{enumerate}
	\item Для SGD:
	\begin{enumerate}
		\item $\alpha = 0.1$
		\item  $\beta = 0.0347$
		\item  $\omega_0 = 0 \in \mathbb{R}^D$
		\item  $batch\_size = 10000 $
	\end{enumerate}
\end{itemize}
В этом эксперименте рассматривалось влияние на  время работы градиентного метода, $accuracy$, и размерность признакового пространства $D$ таких методов предобработки как:
\begin{itemize}
	\item лемматизация(сведение слово к начальной форме, к лемме). Пример:
	"write wrote written" \ $\rightarrow$ "write write write"
	\item удаление стоп-слов (предлоги, частицы и другие шумовые слова). Пример:
	"The Moscow is the best city" \ $\rightarrow$ "Moscow best city"
\end{itemize}
Для определения влияния обработки рассматривались время, $accuracy$, размер признакового пространства по выборке до обработки и те же самые параметры по выборке после обработки. Сравнение велось как для SGD, так и для GD.
\subsubsection{Результаты эксперимента}
\begin{tabular}{|c|c|c|c|}
	\hline
		& Время (с)  & Количество признаков & $accuracy$ \\
	\hline
		GD без обработки & 8.925143 &  16050 & 0.806100 \\
	\hline
		GD с обработкой & 5.081007 & 12964 & 0.820700 \\
	\hline
		SGD без обработки & 11.928005 &  16050 & 0.814100 \\
	\hline
		SGD с обработкой & 10.939810 & 12964 & 0.830700 \\
	\hline
\end{tabular}\\

Сразу заметим, что размерность признакового пространства сильно уменьшилась. Из-за уменьшения размерности пространства с учетом вычислительной сложности для GD и SGD, полученной выше в эксперименте 3, очевидно, уменьшится и время работы модели, что подтверждается значениями в таблице. Из таблицы также следует, что лемматизация и удаление стоп-слов способствует значительному улучшению точности: для GD на 7.52\% меньше ошибок (на $\approx 145$ меньше текстов-ошибок для валидационной выборки размера 10000), а для SGD на 8.92\% меньше ошибок (на $\approx 165$ меньше текстов-ошибок для валидационной выборки размера 10000). 
\subsubsection{Выводы эксперимента}
Предобработка корпуса с помощью лемматизации и последующего удаления стоп-слов дает не только выигрыш в памяти, но и позволяет улучшить модель по времени и точности.
