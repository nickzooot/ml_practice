\section{\huge Список экспериментов}
Перед проведением экспериментов была проведена предобработка текстов: приводились слова к нижнему регистру, не буквы и не цифры удалялись и заменялись на пробелы.

Обучающая выборка была перемешена и разделена на валидационную и обучающую в отношении 1 : 4.

 Значения некоторых параметров не изменялись почти во всех экспериментах. Если эти значения будут меняться, то это будет отдельно оговорено. Перечислим значения этих параметров по умолчанию:
\begin{itemize}
	\item 
	Максимальное количество итераций(эпох в случае стохастического градиентного спуска)[{\sl max\_iters}]\footnote{Далее для сокращения записи в случае {\itshape градиентного спуска} будем писать {\bfseries GD}, а в случае {\itshape стохастического градиентного спуска} {\bfseries SGD}}: 100
	\item  
	Точность[{\sl tolerance}]: $10^{-10}$
	\item
	Коэффициент $l_2$-регуляризации: 0.1
	\item Размер батча(подвыборки для подсчета градиента в SGD)[{\sl batch\_size}]:~10000 
	\item Частота обновлений "истории"(время, accuracy и значение функции потерь) SGD: 1 эпоха
\end{itemize}

Рассмотрим формулу итерационного нахождения минимума:
\begin{equation}
	\omega_{k+1} = \omega_k - \frac{1}{L}\eta_k\sum_{i=1}^L\bigtriangledown_\omega Q(X,y,\omega_k),
\label{grad}
\end{equation}
где $L$ - это либо {\sl batch\_size}, либо размер выборки $X$.

В качестве темпа обучения({\itshape learning rate}) бралась 
\begin{equation}
	\eta_k =\frac{\alpha}{k^\beta}\label{step}
\end{equation}